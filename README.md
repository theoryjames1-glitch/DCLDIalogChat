Perfect â€” letâ€™s build a **Generative Coevolutionary Adversarial Learning (GCAL) / Differentiable Coevolutionary Learning (DCL)** theory tailored to **DialoGPT**.

---

# ðŸ“œ GCAL/DCL Theory for DialoGPT

---

## 1. Motivation

* Traditional **language models** (GPT, DialoGPT) are trained once, offline, to minimize next-token prediction.
* They **do not evolve** after deployment, and they optimize only against static text corpora.
* Our goal: create a **coevolutionary learning system** where conversational agents **adapt forever** by interacting with each other.

---

## 2. Framework

We define two populations of dialogue agents:

* **Agent A (Competitor)** â†’ tries to generate coherent, engaging responses.
* **Agent B (Adversary / Distorter)** â†’ perturbs, challenges, or destabilizes the conversation.
* Both are based on **DialoGPT**, initialized identically, but updated with different pressures.

### Conversation Loop

1. Agent A says something.
2. Agent B responds.
3. Conversation continues indefinitely.

History = {utterances Uâ‚, Uâ‚‚, â€¦, Uâ‚œ}, treated as differentiable input.

---

## 3. Learning Signals

Agents are not supervised with labels â€” they **learn from each other**.
Two key differentiable objectives:

1. **Perplexity Cross-Fitness**

   * Each agent is rewarded when the other can *predict* its response (low perplexity).
   * Equivalent to: â€œIf I speak clearly, you can model me.â€
   * LossA = Perplexity(B|A), LossB = Perplexity(A|B).

2. **Reinforcement Coevolution**

   * Additional reward shaping:

     * Agent A gets reward for *winning coherence* (fluency, grammar, similarity to human dialogs).
     * Agent B gets reward for *forcing novelty* or *breaking prediction patterns*.
   * Can be implemented with REINFORCE on sampled replies, using entropy + coherence scoring.

---

## 4. GCAL Dynamics

This creates a **Generative Coevolutionary Adversarial Loop**:

* Agent A wants to **minimize chaos** (make sense).
* Agent B wants to **maximize chaos** (destabilize or distort).
* Together, they co-adapt in an **arms race**:

  * A becomes more robust against nonsense.
  * B becomes more clever at introducing novelty.

This is analogous to **GANs**, but at the level of **dialogue interaction** rather than image synthesis.

---

## 5. DCL DialoGPT

Differentiable Coevolutionary Learning (DCL) provides the gradient machinery:

* Population = {weights of Agent A, Agent B}.
* Selection = **soft reweighting** based on conversation losses.
* Variation = **gradient-based mutation** (Adafactor, Adam).
* Evolution = continuous adaptation as dialogue unfolds.

This avoids discrete GA-style mutations and allows **backpropagation through conversation**.

---

## 6. Emergent Phenomena

In long-running coevolution:

* **Stability of the Adaptive**: conversations tend toward equilibrium where neither can dominate (like perpetual draws in Tic-Tac-Toe).
* **Strange Attractors**: sometimes dialogue collapses into loops, jargon, or â€œkookooâ€ states (pathological attractors).
* **Creative Drift**: because B pressures A with destabilization, A develops robustness and originality.

---

## 7. Relationship to Other Theories

* **Vs. Classical Minimax Games**: Minimax seeks stable equilibria. GCAL/DCL instead seeks *perpetual adaptation*.
* **Vs. Bandits**: Bandits learn optimal actions; DCL learns **adaptive strategies that shift as opponents evolve**.
* **Vs. GANs**: GAN = generator vs discriminator; GCAL/DCL = speaker vs adversarial speaker.
* **Vs. Standard Language Modeling**: LM training is static; DCL-DialoGPT is **open-ended, evolving forever**.

---

## 8. Applications

* **Self-Improving Chatbots**: Bots that refine conversation quality through endless self-play.
* **Robust Dialogue Agents**: Models resilient to adversarial prompts and destabilization.
* **Artificial Societies**: Populations of chatbots evolving dialects, memes, and strategies.
* **Creative Generation**: Coevolution drives novelty beyond static corpora.

---

âœ… **In short**:
**GCAL/DCL for DialoGPT reframes dialogue as an evolving ecosystem of conversational agents, where coherence and disruption coevolve, leading to perpetual learning and adaptive robustness.**

---

Great âš¡ â€” letâ€™s design a **mini demo** of **GCAL/DCL for DialoGPT** that you can run and extend.
Weâ€™ll set up:

* **Agent A (Competitor):** tries to be predictable & coherent.
* **Agent B (Distorter):** tries to be less predictable (forcing A to adapt).
* **Perplexity loss (cross-fitness):** each agentâ€™s response is scored by the other.
* **Reinforce-style reward:** Distorter gets bonus for being *hard to predict* (higher perplexity).

---

# ðŸ§© `DCLDialoGPT.py`

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, Adafactor
import torch

# ------------------ Setup ------------------
model_name = "microsoft/DialoGPT-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Competitor (A): trainable
agentA = AutoModelForCausalLM.from_pretrained(model_name)
optA = Adafactor(agentA.parameters(), lr=5e-5, relative_step=False, scale_parameter=True)

# Distorter (B): trainable
agentB = AutoModelForCausalLM.from_pretrained(model_name)
optB = Adafactor(agentB.parameters(), lr=5e-5, relative_step=False, scale_parameter=True)


def generate(agent, history, max_new=40):
    """Generate a reply from an agent given dialogue history."""
    inputs = tokenizer.encode(history + tokenizer.eos_token, return_tensors="pt")
    outputs = agent.generate(
        inputs,
        max_length=inputs.shape[1] + max_new,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_k=50
    )
    return tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)


def dialogue_step(history):
    """One turn: A speaks, then B responds. Returns losses + updated history."""
    # Agent A reply
    replyA = generate(agentA, history)
    idsA = tokenizer.encode(replyA, return_tensors="pt")
    lossB = agentB(idsA, labels=idsA).loss  # B must predict A
    history += " " + replyA

    # Agent B reply
    replyB = generate(agentB, history)
    idsB = tokenizer.encode(replyB, return_tensors="pt")
    lossA = agentA(idsB, labels=idsB).loss  # A must predict B
    history += " " + replyB

    return replyA, replyB, lossA, lossB, history


# ------------------ Training Loop ------------------
history = "Hello"
episodes = 50

for ep in range(1, episodes + 1):
    replyA, replyB, lossA, lossB, history = dialogue_step(history)

    # Competitor (A) update â†’ minimize perplexity of B
    optA.zero_grad()
    lossA.backward(retain_graph=True)
    optA.step()

    # Distorter (B) update â†’ maximize perplexity of A (so minimize -lossB)
    optB.zero_grad()
    (-lossB).backward()
    optB.step()

    if ep % 5 == 0:
        print(f"\nEpisode {ep:03d}")
        print(f"A: {replyA}")
        print(f"B: {replyB}")
        print(f"LossA={lossA.item():.4f} | LossB={lossB.item():.4f}")
```

---

## ðŸ” How It Works

* **Agent A (Competitor)** â†’ updated to minimize `lossA` (wants to model B well).
* **Agent B (Distorter)** â†’ updated to maximize Aâ€™s loss (wants to be hard to model).
* **Dialogue history** grows turn by turn, so context carries forward.
* Every few episodes you see the conversation evolve.

---

## âš¡ What to Expect

* At first: generic chit-chat.
* After training:

  * **A** will try to keep things coherent & predictable.
  * **B** will drift into stranger utterances that break predictability.
  * Sometimes, history drifts into loops or â€œkookoo attractors.â€

---

âš¡ Yes James â€” thatâ€™s *exactly* the kind of next step that makes this exciting: we can take the **algorithms in the Clever Algorithms PDF** (Genetic Algorithms, PSO, Immune Systems, SA, etc.) and reinterpret them in the **Differentiable Coevolutionary Learning (DCL) / Generative Coevolutionary Adversarial Learning (GCAL)** framework for **DialoGPT dialogue training**.

Letâ€™s sketch how:

---

# ðŸ“œ Using Clever Algorithms Theories for **DCLDialoChat**

---

## 1. **Evolutionary Algorithms â†’ Differentiable Dialogue Evolution**

* Classic: population of dialogues, mutated and selected.
* DCL version:

  * Population = multiple candidate utterances (tensors).
  * Variation = gradient-based perturbations of responses.
  * Selection = softmax weighting of utterances based on coherence (perplexity) + novelty reward.
* Result: dialogue evolves into **more stable yet adaptive conversations**.

---

## 2. **Swarm Intelligence â†’ Swarm Conversations**

* Classic PSO/ACO: swarm shares best-found solutions.
* DCL version:

  * Multiple dialogue agents act like a â€œswarm,â€ each exploring a different conversational branch.
  * Agents exchange gradients instead of pheromones.
  * â€œBest utteranceâ€ influences the next wave of conversation.
* Result: conversation that doesnâ€™t collapse, but **self-organizes around attractors**.

---

## 3. **Immune Algorithms â†’ Dialect Immunity**

* Classic: immune system distinguishes self/non-self, clones antibodies.
* DCL version:

  * â€œSelfâ€ = coherent, on-topic replies.
  * â€œNon-selfâ€ = adversarial, destabilizing replies.
  * Agents evolve antibodies (responses) against adversarial distortions.
* Result: chatbot develops **robustness against kookoo attractors** or malicious prompts.

---

## 4. **Simulated Annealing â†’ Annealed Dialogue Search**

* Classic: SA uses temperature to escape local optima.
* DCL version:

  * Temperature = randomness in sampling tokens.
  * Cooling schedule = adaptive entropy reduction.
  * Dialogue agents anneal responses: start chaotic, become sharper as training continues.
* Result: conversations that **explore creativity early, stabilize later**.

---

## 5. **Harmony Search / Cultural Algorithms â†’ Collective Dialogue**

* Classic: harmony search optimizes melodies; cultural algorithms evolve shared knowledge.
* DCL version:

  * Shared memory = â€œcultural normsâ€ of conversation (grammar, tone).
  * Agents coevolve deviations (Distorter) and corrections (Competitor).
* Result: bots learn **language norms and drift-resistant dialogue styles**.

---

# ðŸ”¬ How This Applies to DCLDialoChat

* **Competitor** = keeps conversation grounded (minimize perplexity).
* **Distorter** = introduces novelty / destabilization (maximize perplexity or entropy).
* Algorithms from the PDF provide **different styles of adaptation**:

  * GA-style selection of utterances.
  * PSO-style swarm exploration of dialogue branches.
  * Immune-style robustness against adversarial distortion.
  * SA-style annealed exploration â†’ stability.

Instead of just using one fixed reward, you can **rotate through these theories** to pressure the agents in different ways.

---

Perfect âš¡ James â€” hereâ€™s a **mapping table** that ties the **Clever Algorithms book** to **DCL/GCAL DialoGPT** training, showing how each theory could shape dialogue learning.

---

# ðŸ“Š Clever Algorithms â†’ DCLDialoChat Adaptation

| **Algorithm Family**                  | **Classical Idea**                                   | **DCLDialoChat Adaptation**                                                                               | **Expected Conversational Behavior**                                      |
| ------------------------------------- | ---------------------------------------------------- | --------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------- |
| **Genetic Algorithms**                | Population evolves via mutation/crossover, selection | Generate multiple candidate replies, perturb weights with gradient noise, select via softmax on coherence | Conversation gradually â€œevolvesâ€ toward more coherent, human-like replies |
| **Evolution Strategies**              | Adapt strategy parameters, self-adjust mutation      | Agents adjust **temperature / entropy** of sampling via gradients                                         | More stable, adaptive dialogue style (less collapse, balanced creativity) |
| **Differential Evolution**            | Combine differences of population vectors            | Combine differences of reply embeddings to generate new utterances                                        | Hybrid replies emerge, exploring novel conversational space               |
| **Particle Swarm Optimization (PSO)** | Swarm explores space, influenced by best solutions   | Agents act like a swarm of conversationalists, share best utterances                                      | Dialogue self-organizes, avoids dead ends                                 |
| **Ant Colony Optimization (ACO)**     | Paths reinforced by pheromones                       | Reward certain reply patterns (e.g. semantic coherence) as â€œpheromone trailsâ€                             | Strong conversational motifs emerge, less random chatter                  |
| **Immune Algorithms**                 | Distinguish self/non-self, evolve antibodies         | Distorter = adversarial prompt; Competitor evolves â€œantibody repliesâ€                                     | Robustness against adversarial prompts or kookoo attractors               |
| **Simulated Annealing**               | Random exploration with cooling schedule             | Start with high randomness in replies, reduce entropy over time                                           | Conversations start playful, then stabilize into focused topics           |
| **Guided Local Search**               | Penalize repeated features to escape local optima    | Penalize repetitive utterances (â€œI donâ€™t knowâ€, loops)                                                    | Dialogue avoids boring repetition                                         |
| **Clonal Selection**                  | Clone best antibodies, mutate them                   | Clone best replies, add gradient noise mutations                                                          | Rich variation of coherent responses                                      |
| **Boltzmann Machine / Hopfield Net**  | Energy minimization, associative memory              | Dialogue history as an energy landscape; minimize â€œconfusion energyâ€                                      | Coherent long-term memory in conversation                                 |
| **Cultural Algorithms**               | Evolution with shared cultural knowledge             | Shared â€œbelief spaceâ€ of conversational norms (tone, grammar)                                             | Bots develop **shared style/language**, e.g. slang or dialect             |
| **Harmony Search**                    | Musical harmony optimization                         | Multiple agents â€œjamâ€ with dialogue, balancing novelty and fit                                            | Dialogue feels more like improvisational play, creative exchanges         |
| **Memetic Algorithms**                | Combine GA with local search                         | Evolve conversation structure, refine locally with gradient descent                                       | Convos improve both globally (evolution) and locally (fine-tuning)        |

---

# ðŸ§© Key Insight

* Each algorithm **translates into a conversational learning pressure**.
* Instead of just perplexity/reward, we can plug in **different algorithmic metaphors** to guide the coevolution.
* Together, this becomes a **meta-algorithm zoo** for **open-ended dialogue learning**.

âš¡ Yes James â€” adding **Asymmetry** is exactly how we make GCAL/DCL richer and closer to reality. Right now, most formulations (like GA, PSO, etc.) assume **symmetry**: both agents optimize under the same rules. But in conversations (and in games, evolution, or adversarial systems) the power balance is rarely symmetric.

---

# ðŸ“œ Theory of Asymmetric GCAL/DCL

---

## 1. Symmetry vs Asymmetry

* **Symmetric Coevolution**: both agents share the same objectives, update rules, and constraints (e.g., predator â†” prey with equal speed).
* **Asymmetric Coevolution**: each agent has **different objectives, resources, or update rules**.

  * Competitor tries to **minimize perplexity**.
  * Distorter tries to **maximize unpredictability**.
  * Agents may have different **learning rates**, **update frequencies**, or **abilities**.

---

## 2. Asymmetric Roles in DCLDialoChat

* **Competitor (Agent A):**

  * Goal: coherence, clarity, human-likeness.
  * Loss: minimize perplexity of its outputs for B.
  * Behavior: conservative, stabilizing force.

* **Distorter (Agent B):**

  * Goal: destabilize Aâ€™s modeling, push novelty.
  * Loss: maximize perplexity A experiences when predicting B.
  * Behavior: exploratory, chaotic force.

---

## 3. Asymmetric Learning Dynamics

We can encode asymmetry at multiple levels:

1. **Learning Rates**

   * Competitor: slow and stable (Î·=1e-5).
   * Distorter: fast and chaotic (Î·=1e-4).

2. **Update Frequency**

   * Competitor updates every turn.
   * Distorter updates only sometimes (burst-like attacks).

3. **Reward Shaping**

   * Competitor gets high reward for consistency.
   * Distorter gets high reward for forcing resets (when convo breaks).

4. **Resource Asymmetry**

   * Competitor has more memory (longer context window).
   * Distorter has less memory (shorter-term tactics).

---

## 4. Expected Emergent Behaviors

* **Arms Race:** Competitor stabilizes â†’ Distorter invents new perturbations â†’ Competitor adapts.
* **Strange Attractors:** Sometimes conversation collapses into nonsense, showing chaotic attractors.
* **Adaptive Equilibria:** System never converges, but oscillates between **clarity** and **novelty chaos**.
* **Creative Drift:** Out of adversarial asymmetry, unexpected **language creativity** emerges.

---

## 5. Relationship to Clever Algorithms

Asymmetry adds another layer to the **algorithm zoo** from the PDF:

* GA with asymmetric mutation rates.
* Swarm with leaders/followers.
* Immune system with attackers vs defenders.
* Annealing with one agent cooling faster than the other.

---

## 6. DialoGPT Implementation

In practice, asymmetry means:

* `optA` and `optB` have different settings.
* Loss functions differ (competitor = minimize perplexity; distorter = maximize perplexity + entropy).
* Training loop introduces **imbalance** to encourage rich dynamics.

---

âœ… **In summary:**
**Asymmetric GCAL/DCL** reframes dialogue as a **conflict between stabilizers and disruptors**.

* Without asymmetry â†’ system converges to dull equilibrium.
* With asymmetry â†’ system sustains **endless adaptive evolution**.

---

Perfect âš¡ James â€” letâ€™s build an **Asymmetric DCLDialoGPT demo**.
Weâ€™ll make **Agent A (Competitor)** conservative + stabilizing, while **Agent B (Distorter)** is chaotic + destabilizing.

---

# ðŸ§© `AsymDCLDialoGPT.py`

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, Adafactor
import torch

# ------------------ Setup ------------------
model_name = "microsoft/DialoGPT-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Competitor (Agent A): coherent stabilizer
agentA = AutoModelForCausalLM.from_pretrained(model_name)
optA = Adafactor(agentA.parameters(), lr=2e-5, relative_step=False, scale_parameter=True)

# Distorter (Agent B): chaotic disruptor
agentB = AutoModelForCausalLM.from_pretrained(model_name)
optB = Adafactor(agentB.parameters(), lr=1e-4, relative_step=False, scale_parameter=True)


def generate(agent, history, max_new=40, temperature=0.9):
    """Generate reply with some randomness (Distorter gets higher temperature)."""
    inputs = tokenizer.encode(history + tokenizer.eos_token, return_tensors="pt")
    outputs = agent.generate(
        inputs,
        max_length=inputs.shape[1] + max_new,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_k=50,
        temperature=temperature
    )
    return tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)


def dialogue_step(history):
    """One turn: A speaks, then B responds. Returns losses + updated history."""
    # Agent A reply (stable, low temperature)
    replyA = generate(agentA, history, temperature=0.7)
    idsA = tokenizer.encode(replyA, return_tensors="pt")
    lossB = agentB(idsA, labels=idsA).loss  # B must predict A
    history += " " + replyA

    # Agent B reply (chaotic, high temperature)
    replyB = generate(agentB, history, temperature=1.2)
    idsB = tokenizer.encode(replyB, return_tensors="pt")
    lossA = agentA(idsB, labels=idsB).loss  # A must predict B
    history += " " + replyB

    return replyA, replyB, lossA, lossB, history


# ------------------ Training Loop ------------------
history = "Hello"
episodes = 30

for ep in range(1, episodes + 1):
    replyA, replyB, lossA, lossB, history = dialogue_step(history)

    # Competitor (A): minimize perplexity (stabilizing force)
    optA.zero_grad()
    lossA.backward(retain_graph=True)
    optA.step()

    # Distorter (B): maximize perplexity (destabilizing force)
    # update less often (asymmetric frequency)
    if ep % 2 == 0:  
        optB.zero_grad()
        (-lossB + 0.1 * lossA).backward()  
        # -lossB â†’ maximize A's difficulty
        # +0.1*lossA â†’ bonus for forcing resets/chaos
        optB.step()

    if ep % 5 == 0:
        print(f"\nEpisode {ep:03d}")
        print(f"A (Competitor): {replyA}")
        print(f"B (Distorter): {replyB}")
        print(f"LossA={lossA.item():.4f} | LossB={lossB.item():.4f}")
```

---

## ðŸ” What Changed

* **Asymmetry in learning rate:**

  * A = 2e-5 (stable, slow).
  * B = 1e-4 (fast, chaotic).

* **Asymmetry in generation style:**

  * A = temperature 0.7 (predictable).
  * B = temperature 1.2 (unpredictable).

* **Asymmetry in update frequency:**

  * A updates every step.
  * B updates only on even steps.

* **Asymmetry in rewards:**

  * A minimizes perplexity (`lossA`).
  * B maximizes Aâ€™s perplexity (`-lossB`) and gets bonus when A struggles (`+0.1*lossA`).

---

## âš¡ What to Expect

* Conversations where A tries to stay coherent.
* B sometimes goes off-track, introducing chaos.
* Dynamic **arms race** â†’ A stabilizes, B distorts.
* Emergent **kookoo attractors** appear, but A adapts to resist.

---

ðŸ‘‰ Do you want me to **add a memory reset mechanism** (e.g., only keep last 2 turns) so the convo doesnâ€™t drift too far into nonsense but still allows â€œchaos burstsâ€?


### PSUEDOCODES
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, Adafactor
import torch

# --- Setup ---
model_name = "microsoft/DialoGPT-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)

agentA = AutoModelForCausalLM.from_pretrained(model_name)
agentB = AutoModelForCausalLM.from_pretrained(model_name)

optA = Adafactor(agentA.parameters(), lr=5e-5, relative_step=False, scale_parameter=True)
optB = Adafactor(agentB.parameters(), lr=5e-5, relative_step=False, scale_parameter=True)

# --- Generate reply with min length & fallback ---
def generate_reply(agent, history, max_len=40, min_len=3):
    enc = tokenizer(history + tokenizer.eos_token, return_tensors="pt", truncation=True)
    ids = enc["input_ids"]
    mask = enc["attention_mask"]

    output = agent.generate(
        ids,
        attention_mask=mask,
        max_length=ids.shape[-1] + max_len,
        min_length=ids.shape[-1] + min_len,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_k=50,
    )

    reply = tokenizer.decode(output[0][ids.shape[-1]:], skip_special_tokens=True)

    if not reply.strip():  # fallback
        reply = "..."

    return reply.strip()

# --- Dialogue Step with Reward Shaping ---
def dialogue_step(agentA, agentB, history):
    last = history[-1]
    
    replyA = generate_reply(agentA, last)
    history += replyA + tokenizer.eos_token

    replyB = generate_reply(agentB, replyA)
    history += replyB + tokenizer.eos_token

    lossA, lossB = None, None
    rewardA, rewardB = 0.0, 0.0

    # Helper for shaping
    def shaped_reward(reply, base_loss):
        reward = -base_loss.item()
        length_bonus = min(len(reply.split()), 10) * 0.05
        tokens = reply.lower().split()
        unique_ratio = len(set(tokens)) / (len(tokens) + 1e-6)
        repetition_penalty = - (1 - unique_ratio) * 0.5
        return reward + length_bonus + repetition_penalty

    # Aâ€™s reply judged by B
    if replyA.strip():
        idsA = tokenizer.encode(replyA, return_tensors="pt")
        if idsA.shape[-1] > 0:
            lossB = agentA(idsA, labels=idsA).loss
            rewardB = shaped_reward(replyA, lossB)

    # Bâ€™s reply judged by A
    if replyB.strip():
        idsB = tokenizer.encode(replyB, return_tensors="pt")
        if idsB.shape[-1] > 0:
            lossA = agentB(idsB, labels=idsB).loss
            rewardA = shaped_reward(replyB, lossA)

    return replyA, replyB, lossA, lossB, rewardA, rewardB, history

# --- Training Loop ---
history = "Hello" + tokenizer.eos_token
baselineA, baselineB = 0.0, 0.0

for step in range(20):
    replyA, replyB, lossA, lossB, rewardA, rewardB, history = dialogue_step(agentA, agentB, history)

    if lossA is not None:
        baselineA = 0.9 * baselineA + 0.1 * rewardA
        optA.zero_grad()
        (lossA * (rewardA - baselineA)).backward()
        torch.nn.utils.clip_grad_norm_(agentA.parameters(), 1.0)
        optA.step()

    if lossB is not None:
        baselineB = 0.9 * baselineB + 0.1 * rewardB
        optB.zero_grad()
        (lossB * (rewardB - baselineB)).backward()
        torch.nn.utils.clip_grad_norm_(agentB.parameters(), 1.0)
        optB.step()

    print(f"Step {step:02d}")
    print(f" A: {replyA}")
    print(f" B: {replyB}")
    print(f" RewardA={rewardA:.4f} | RewardB={rewardB:.4f}\n")

    # Trim history to avoid GPU blow-up
    if len(history) > 500:
        history = history[1:-1]
```
