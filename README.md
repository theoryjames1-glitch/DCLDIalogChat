Perfect ‚Äî let‚Äôs build a **Generative Coevolutionary Adversarial Learning (GCAL) / Differentiable Coevolutionary Learning (DCL)** theory tailored to **DialoGPT**.

---

# üìú GCAL/DCL Theory for DialoGPT

---

## 1. Motivation

* Traditional **language models** (GPT, DialoGPT) are trained once, offline, to minimize next-token prediction.
* They **do not evolve** after deployment, and they optimize only against static text corpora.
* Our goal: create a **coevolutionary learning system** where conversational agents **adapt forever** by interacting with each other.

---

## 2. Framework

We define two populations of dialogue agents:

* **Agent A (Competitor)** ‚Üí tries to generate coherent, engaging responses.
* **Agent B (Adversary / Distorter)** ‚Üí perturbs, challenges, or destabilizes the conversation.
* Both are based on **DialoGPT**, initialized identically, but updated with different pressures.

### Conversation Loop

1. Agent A says something.
2. Agent B responds.
3. Conversation continues indefinitely.

History = {utterances U‚ÇÅ, U‚ÇÇ, ‚Ä¶, U‚Çú}, treated as differentiable input.

---

## 3. Learning Signals

Agents are not supervised with labels ‚Äî they **learn from each other**.
Two key differentiable objectives:

1. **Perplexity Cross-Fitness**

   * Each agent is rewarded when the other can *predict* its response (low perplexity).
   * Equivalent to: ‚ÄúIf I speak clearly, you can model me.‚Äù
   * LossA = Perplexity(B|A), LossB = Perplexity(A|B).

2. **Reinforcement Coevolution**

   * Additional reward shaping:

     * Agent A gets reward for *winning coherence* (fluency, grammar, similarity to human dialogs).
     * Agent B gets reward for *forcing novelty* or *breaking prediction patterns*.
   * Can be implemented with REINFORCE on sampled replies, using entropy + coherence scoring.

---

## 4. GCAL Dynamics

This creates a **Generative Coevolutionary Adversarial Loop**:

* Agent A wants to **minimize chaos** (make sense).
* Agent B wants to **maximize chaos** (destabilize or distort).
* Together, they co-adapt in an **arms race**:

  * A becomes more robust against nonsense.
  * B becomes more clever at introducing novelty.

This is analogous to **GANs**, but at the level of **dialogue interaction** rather than image synthesis.

---

## 5. DCL DialoGPT

Differentiable Coevolutionary Learning (DCL) provides the gradient machinery:

* Population = {weights of Agent A, Agent B}.
* Selection = **soft reweighting** based on conversation losses.
* Variation = **gradient-based mutation** (Adafactor, Adam).
* Evolution = continuous adaptation as dialogue unfolds.

This avoids discrete GA-style mutations and allows **backpropagation through conversation**.

---

## 6. Emergent Phenomena

In long-running coevolution:

* **Stability of the Adaptive**: conversations tend toward equilibrium where neither can dominate (like perpetual draws in Tic-Tac-Toe).
* **Strange Attractors**: sometimes dialogue collapses into loops, jargon, or ‚Äúkookoo‚Äù states (pathological attractors).
* **Creative Drift**: because B pressures A with destabilization, A develops robustness and originality.

---

## 7. Relationship to Other Theories

* **Vs. Classical Minimax Games**: Minimax seeks stable equilibria. GCAL/DCL instead seeks *perpetual adaptation*.
* **Vs. Bandits**: Bandits learn optimal actions; DCL learns **adaptive strategies that shift as opponents evolve**.
* **Vs. GANs**: GAN = generator vs discriminator; GCAL/DCL = speaker vs adversarial speaker.
* **Vs. Standard Language Modeling**: LM training is static; DCL-DialoGPT is **open-ended, evolving forever**.

---

## 8. Applications

* **Self-Improving Chatbots**: Bots that refine conversation quality through endless self-play.
* **Robust Dialogue Agents**: Models resilient to adversarial prompts and destabilization.
* **Artificial Societies**: Populations of chatbots evolving dialects, memes, and strategies.
* **Creative Generation**: Coevolution drives novelty beyond static corpora.

---

‚úÖ **In short**:
**GCAL/DCL for DialoGPT reframes dialogue as an evolving ecosystem of conversational agents, where coherence and disruption coevolve, leading to perpetual learning and adaptive robustness.**

---

Great ‚ö° ‚Äî let‚Äôs design a **mini demo** of **GCAL/DCL for DialoGPT** that you can run and extend.
We‚Äôll set up:

* **Agent A (Competitor):** tries to be predictable & coherent.
* **Agent B (Distorter):** tries to be less predictable (forcing A to adapt).
* **Perplexity loss (cross-fitness):** each agent‚Äôs response is scored by the other.
* **Reinforce-style reward:** Distorter gets bonus for being *hard to predict* (higher perplexity).

---

# üß© `DCLDialoGPT.py`

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, Adafactor
import torch

# ------------------ Setup ------------------
model_name = "microsoft/DialoGPT-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Competitor (A): trainable
agentA = AutoModelForCausalLM.from_pretrained(model_name)
optA = Adafactor(agentA.parameters(), lr=5e-5, relative_step=False, scale_parameter=True)

# Distorter (B): trainable
agentB = AutoModelForCausalLM.from_pretrained(model_name)
optB = Adafactor(agentB.parameters(), lr=5e-5, relative_step=False, scale_parameter=True)


def generate(agent, history, max_new=40):
    """Generate a reply from an agent given dialogue history."""
    inputs = tokenizer.encode(history + tokenizer.eos_token, return_tensors="pt")
    outputs = agent.generate(
        inputs,
        max_length=inputs.shape[1] + max_new,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_k=50
    )
    return tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)


def dialogue_step(history):
    """One turn: A speaks, then B responds. Returns losses + updated history."""
    # Agent A reply
    replyA = generate(agentA, history)
    idsA = tokenizer.encode(replyA, return_tensors="pt")
    lossB = agentB(idsA, labels=idsA).loss  # B must predict A
    history += " " + replyA

    # Agent B reply
    replyB = generate(agentB, history)
    idsB = tokenizer.encode(replyB, return_tensors="pt")
    lossA = agentA(idsB, labels=idsB).loss  # A must predict B
    history += " " + replyB

    return replyA, replyB, lossA, lossB, history


# ------------------ Training Loop ------------------
history = "Hello"
episodes = 50

for ep in range(1, episodes + 1):
    replyA, replyB, lossA, lossB, history = dialogue_step(history)

    # Competitor (A) update ‚Üí minimize perplexity of B
    optA.zero_grad()
    lossA.backward(retain_graph=True)
    optA.step()

    # Distorter (B) update ‚Üí maximize perplexity of A (so minimize -lossB)
    optB.zero_grad()
    (-lossB).backward()
    optB.step()

    if ep % 5 == 0:
        print(f"\nEpisode {ep:03d}")
        print(f"A: {replyA}")
        print(f"B: {replyB}")
        print(f"LossA={lossA.item():.4f} | LossB={lossB.item():.4f}")
```

---

## üîç How It Works

* **Agent A (Competitor)** ‚Üí updated to minimize `lossA` (wants to model B well).
* **Agent B (Distorter)** ‚Üí updated to maximize A‚Äôs loss (wants to be hard to model).
* **Dialogue history** grows turn by turn, so context carries forward.
* Every few episodes you see the conversation evolve.

---

## ‚ö° What to Expect

* At first: generic chit-chat.
* After training:

  * **A** will try to keep things coherent & predictable.
  * **B** will drift into stranger utterances that break predictability.
  * Sometimes, history drifts into loops or ‚Äúkookoo attractors.‚Äù

---

‚ö° Yes James ‚Äî that‚Äôs *exactly* the kind of next step that makes this exciting: we can take the **algorithms in the Clever Algorithms PDF** (Genetic Algorithms, PSO, Immune Systems, SA, etc.) and reinterpret them in the **Differentiable Coevolutionary Learning (DCL) / Generative Coevolutionary Adversarial Learning (GCAL)** framework for **DialoGPT dialogue training**.

Let‚Äôs sketch how:

---

# üìú Using Clever Algorithms Theories for **DCLDialoChat**

---

## 1. **Evolutionary Algorithms ‚Üí Differentiable Dialogue Evolution**

* Classic: population of dialogues, mutated and selected.
* DCL version:

  * Population = multiple candidate utterances (tensors).
  * Variation = gradient-based perturbations of responses.
  * Selection = softmax weighting of utterances based on coherence (perplexity) + novelty reward.
* Result: dialogue evolves into **more stable yet adaptive conversations**.

---

## 2. **Swarm Intelligence ‚Üí Swarm Conversations**

* Classic PSO/ACO: swarm shares best-found solutions.
* DCL version:

  * Multiple dialogue agents act like a ‚Äúswarm,‚Äù each exploring a different conversational branch.
  * Agents exchange gradients instead of pheromones.
  * ‚ÄúBest utterance‚Äù influences the next wave of conversation.
* Result: conversation that doesn‚Äôt collapse, but **self-organizes around attractors**.

---

## 3. **Immune Algorithms ‚Üí Dialect Immunity**

* Classic: immune system distinguishes self/non-self, clones antibodies.
* DCL version:

  * ‚ÄúSelf‚Äù = coherent, on-topic replies.
  * ‚ÄúNon-self‚Äù = adversarial, destabilizing replies.
  * Agents evolve antibodies (responses) against adversarial distortions.
* Result: chatbot develops **robustness against kookoo attractors** or malicious prompts.

---

## 4. **Simulated Annealing ‚Üí Annealed Dialogue Search**

* Classic: SA uses temperature to escape local optima.
* DCL version:

  * Temperature = randomness in sampling tokens.
  * Cooling schedule = adaptive entropy reduction.
  * Dialogue agents anneal responses: start chaotic, become sharper as training continues.
* Result: conversations that **explore creativity early, stabilize later**.

---

## 5. **Harmony Search / Cultural Algorithms ‚Üí Collective Dialogue**

* Classic: harmony search optimizes melodies; cultural algorithms evolve shared knowledge.
* DCL version:

  * Shared memory = ‚Äúcultural norms‚Äù of conversation (grammar, tone).
  * Agents coevolve deviations (Distorter) and corrections (Competitor).
* Result: bots learn **language norms and drift-resistant dialogue styles**.

---

# üî¨ How This Applies to DCLDialoChat

* **Competitor** = keeps conversation grounded (minimize perplexity).
* **Distorter** = introduces novelty / destabilization (maximize perplexity or entropy).
* Algorithms from the PDF provide **different styles of adaptation**:

  * GA-style selection of utterances.
  * PSO-style swarm exploration of dialogue branches.
  * Immune-style robustness against adversarial distortion.
  * SA-style annealed exploration ‚Üí stability.

Instead of just using one fixed reward, you can **rotate through these theories** to pressure the agents in different ways.

---

Perfect ‚ö° James ‚Äî here‚Äôs a **mapping table** that ties the **Clever Algorithms book** to **DCL/GCAL DialoGPT** training, showing how each theory could shape dialogue learning.

---

# üìä Clever Algorithms ‚Üí DCLDialoChat Adaptation

| **Algorithm Family**                  | **Classical Idea**                                   | **DCLDialoChat Adaptation**                                                                               | **Expected Conversational Behavior**                                      |
| ------------------------------------- | ---------------------------------------------------- | --------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------- |
| **Genetic Algorithms**                | Population evolves via mutation/crossover, selection | Generate multiple candidate replies, perturb weights with gradient noise, select via softmax on coherence | Conversation gradually ‚Äúevolves‚Äù toward more coherent, human-like replies |
| **Evolution Strategies**              | Adapt strategy parameters, self-adjust mutation      | Agents adjust **temperature / entropy** of sampling via gradients                                         | More stable, adaptive dialogue style (less collapse, balanced creativity) |
| **Differential Evolution**            | Combine differences of population vectors            | Combine differences of reply embeddings to generate new utterances                                        | Hybrid replies emerge, exploring novel conversational space               |
| **Particle Swarm Optimization (PSO)** | Swarm explores space, influenced by best solutions   | Agents act like a swarm of conversationalists, share best utterances                                      | Dialogue self-organizes, avoids dead ends                                 |
| **Ant Colony Optimization (ACO)**     | Paths reinforced by pheromones                       | Reward certain reply patterns (e.g. semantic coherence) as ‚Äúpheromone trails‚Äù                             | Strong conversational motifs emerge, less random chatter                  |
| **Immune Algorithms**                 | Distinguish self/non-self, evolve antibodies         | Distorter = adversarial prompt; Competitor evolves ‚Äúantibody replies‚Äù                                     | Robustness against adversarial prompts or kookoo attractors               |
| **Simulated Annealing**               | Random exploration with cooling schedule             | Start with high randomness in replies, reduce entropy over time                                           | Conversations start playful, then stabilize into focused topics           |
| **Guided Local Search**               | Penalize repeated features to escape local optima    | Penalize repetitive utterances (‚ÄúI don‚Äôt know‚Äù, loops)                                                    | Dialogue avoids boring repetition                                         |
| **Clonal Selection**                  | Clone best antibodies, mutate them                   | Clone best replies, add gradient noise mutations                                                          | Rich variation of coherent responses                                      |
| **Boltzmann Machine / Hopfield Net**  | Energy minimization, associative memory              | Dialogue history as an energy landscape; minimize ‚Äúconfusion energy‚Äù                                      | Coherent long-term memory in conversation                                 |
| **Cultural Algorithms**               | Evolution with shared cultural knowledge             | Shared ‚Äúbelief space‚Äù of conversational norms (tone, grammar)                                             | Bots develop **shared style/language**, e.g. slang or dialect             |
| **Harmony Search**                    | Musical harmony optimization                         | Multiple agents ‚Äújam‚Äù with dialogue, balancing novelty and fit                                            | Dialogue feels more like improvisational play, creative exchanges         |
| **Memetic Algorithms**                | Combine GA with local search                         | Evolve conversation structure, refine locally with gradient descent                                       | Convos improve both globally (evolution) and locally (fine-tuning)        |

---

# üß© Key Insight

* Each algorithm **translates into a conversational learning pressure**.
* Instead of just perplexity/reward, we can plug in **different algorithmic metaphors** to guide the coevolution.
* Together, this becomes a **meta-algorithm zoo** for **open-ended dialogue learning**.

‚ö° Yes James ‚Äî adding **Asymmetry** is exactly how we make GCAL/DCL richer and closer to reality. Right now, most formulations (like GA, PSO, etc.) assume **symmetry**: both agents optimize under the same rules. But in conversations (and in games, evolution, or adversarial systems) the power balance is rarely symmetric.

---

# üìú Theory of Asymmetric GCAL/DCL

---

## 1. Symmetry vs Asymmetry

* **Symmetric Coevolution**: both agents share the same objectives, update rules, and constraints (e.g., predator ‚Üî prey with equal speed).
* **Asymmetric Coevolution**: each agent has **different objectives, resources, or update rules**.

  * Competitor tries to **minimize perplexity**.
  * Distorter tries to **maximize unpredictability**.
  * Agents may have different **learning rates**, **update frequencies**, or **abilities**.

---

## 2. Asymmetric Roles in DCLDialoChat

* **Competitor (Agent A):**

  * Goal: coherence, clarity, human-likeness.
  * Loss: minimize perplexity of its outputs for B.
  * Behavior: conservative, stabilizing force.

* **Distorter (Agent B):**

  * Goal: destabilize A‚Äôs modeling, push novelty.
  * Loss: maximize perplexity A experiences when predicting B.
  * Behavior: exploratory, chaotic force.

---

## 3. Asymmetric Learning Dynamics

We can encode asymmetry at multiple levels:

1. **Learning Rates**

   * Competitor: slow and stable (Œ∑=1e-5).
   * Distorter: fast and chaotic (Œ∑=1e-4).

2. **Update Frequency**

   * Competitor updates every turn.
   * Distorter updates only sometimes (burst-like attacks).

3. **Reward Shaping**

   * Competitor gets high reward for consistency.
   * Distorter gets high reward for forcing resets (when convo breaks).

4. **Resource Asymmetry**

   * Competitor has more memory (longer context window).
   * Distorter has less memory (shorter-term tactics).

---

## 4. Expected Emergent Behaviors

* **Arms Race:** Competitor stabilizes ‚Üí Distorter invents new perturbations ‚Üí Competitor adapts.
* **Strange Attractors:** Sometimes conversation collapses into nonsense, showing chaotic attractors.
* **Adaptive Equilibria:** System never converges, but oscillates between **clarity** and **novelty chaos**.
* **Creative Drift:** Out of adversarial asymmetry, unexpected **language creativity** emerges.

---

## 5. Relationship to Clever Algorithms

Asymmetry adds another layer to the **algorithm zoo** from the PDF:

* GA with asymmetric mutation rates.
* Swarm with leaders/followers.
* Immune system with attackers vs defenders.
* Annealing with one agent cooling faster than the other.

---

## 6. DialoGPT Implementation

In practice, asymmetry means:

* `optA` and `optB` have different settings.
* Loss functions differ (competitor = minimize perplexity; distorter = maximize perplexity + entropy).
* Training loop introduces **imbalance** to encourage rich dynamics.

---

‚úÖ **In summary:**
**Asymmetric GCAL/DCL** reframes dialogue as a **conflict between stabilizers and disruptors**.

* Without asymmetry ‚Üí system converges to dull equilibrium.
* With asymmetry ‚Üí system sustains **endless adaptive evolution**.

---

Perfect ‚ö° James ‚Äî let‚Äôs build an **Asymmetric DCLDialoGPT demo**.
We‚Äôll make **Agent A (Competitor)** conservative + stabilizing, while **Agent B (Distorter)** is chaotic + destabilizing.

---

# üß© `AsymDCLDialoGPT.py`

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, Adafactor
import torch

# ------------------ Setup ------------------
model_name = "microsoft/DialoGPT-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Competitor (Agent A): coherent stabilizer
agentA = AutoModelForCausalLM.from_pretrained(model_name)
optA = Adafactor(agentA.parameters(), lr=2e-5, relative_step=False, scale_parameter=True)

# Distorter (Agent B): chaotic disruptor
agentB = AutoModelForCausalLM.from_pretrained(model_name)
optB = Adafactor(agentB.parameters(), lr=1e-4, relative_step=False, scale_parameter=True)


def generate(agent, history, max_new=40, temperature=0.9):
    """Generate reply with some randomness (Distorter gets higher temperature)."""
    inputs = tokenizer.encode(history + tokenizer.eos_token, return_tensors="pt")
    outputs = agent.generate(
        inputs,
        max_length=inputs.shape[1] + max_new,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_k=50,
        temperature=temperature
    )
    return tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)


def dialogue_step(history):
    """One turn: A speaks, then B responds. Returns losses + updated history."""
    # Agent A reply (stable, low temperature)
    replyA = generate(agentA, history, temperature=0.7)
    idsA = tokenizer.encode(replyA, return_tensors="pt")
    lossB = agentB(idsA, labels=idsA).loss  # B must predict A
    history += " " + replyA

    # Agent B reply (chaotic, high temperature)
    replyB = generate(agentB, history, temperature=1.2)
    idsB = tokenizer.encode(replyB, return_tensors="pt")
    lossA = agentA(idsB, labels=idsB).loss  # A must predict B
    history += " " + replyB

    return replyA, replyB, lossA, lossB, history


# ------------------ Training Loop ------------------
history = "Hello"
episodes = 30

for ep in range(1, episodes + 1):
    replyA, replyB, lossA, lossB, history = dialogue_step(history)

    # Competitor (A): minimize perplexity (stabilizing force)
    optA.zero_grad()
    lossA.backward(retain_graph=True)
    optA.step()

    # Distorter (B): maximize perplexity (destabilizing force)
    # update less often (asymmetric frequency)
    if ep % 2 == 0:  
        optB.zero_grad()
        (-lossB + 0.1 * lossA).backward()  
        # -lossB ‚Üí maximize A's difficulty
        # +0.1*lossA ‚Üí bonus for forcing resets/chaos
        optB.step()

    if ep % 5 == 0:
        print(f"\nEpisode {ep:03d}")
        print(f"A (Competitor): {replyA}")
        print(f"B (Distorter): {replyB}")
        print(f"LossA={lossA.item():.4f} | LossB={lossB.item():.4f}")
```

---

## üîç What Changed

* **Asymmetry in learning rate:**

  * A = 2e-5 (stable, slow).
  * B = 1e-4 (fast, chaotic).

* **Asymmetry in generation style:**

  * A = temperature 0.7 (predictable).
  * B = temperature 1.2 (unpredictable).

* **Asymmetry in update frequency:**

  * A updates every step.
  * B updates only on even steps.

* **Asymmetry in rewards:**

  * A minimizes perplexity (`lossA`).
  * B maximizes A‚Äôs perplexity (`-lossB`) and gets bonus when A struggles (`+0.1*lossA`).

---

## ‚ö° What to Expect

* Conversations where A tries to stay coherent.
* B sometimes goes off-track, introducing chaos.
* Dynamic **arms race** ‚Üí A stabilizes, B distorts.
* Emergent **kookoo attractors** appear, but A adapts to resist.

---

üëâ Do you want me to **add a memory reset mechanism** (e.g., only keep last 2 turns) so the convo doesn‚Äôt drift too far into nonsense but still allows ‚Äúchaos bursts‚Äù?


### PSUEDOCODES
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, Adafactor
import torch

# --- Setup ---
model_name = "microsoft/DialoGPT-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)

agentA = AutoModelForCausalLM.from_pretrained(model_name)
agentB = AutoModelForCausalLM.from_pretrained(model_name)

optA = Adafactor(agentA.parameters(), lr=5e-5, relative_step=False, scale_parameter=True)
optB = Adafactor(agentB.parameters(), lr=5e-5, relative_step=False, scale_parameter=True)

# --- Generate reply with min length & fallback ---
def generate_reply(agent, history, max_len=40, min_len=3):
    enc = tokenizer(history + tokenizer.eos_token, return_tensors="pt", truncation=True)
    ids = enc["input_ids"]
    mask = enc["attention_mask"]

    output = agent.generate(
        ids,
        attention_mask=mask,
        max_length=ids.shape[-1] + max_len,
        min_length=ids.shape[-1] + min_len,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_k=50,
    )

    reply = tokenizer.decode(output[0][ids.shape[-1]:], skip_special_tokens=True)

    if not reply.strip():  # fallback
        reply = "..."

    return reply.strip()

# --- Dialogue Step with Reward Shaping ---
def dialogue_step(agentA, agentB, history):
    last = history[-1]
    
    replyA = generate_reply(agentA, last)
    history += replyA + tokenizer.eos_token

    replyB = generate_reply(agentB, replyA)
    history += replyB + tokenizer.eos_token

    lossA, lossB = None, None
    rewardA, rewardB = 0.0, 0.0

    # Helper for shaping
    def shaped_reward(reply, base_loss):
        reward = -base_loss.item()
        length_bonus = min(len(reply.split()), 10) * 0.05
        tokens = reply.lower().split()
        unique_ratio = len(set(tokens)) / (len(tokens) + 1e-6)
        repetition_penalty = - (1 - unique_ratio) * 0.5
        return reward + length_bonus + repetition_penalty

    # A‚Äôs reply judged by B
    if replyA.strip():
        idsA = tokenizer.encode(replyA, return_tensors="pt")
        if idsA.shape[-1] > 0:
            lossB = agentA(idsA, labels=idsA).loss
            rewardB = shaped_reward(replyA, lossB)

    # B‚Äôs reply judged by A
    if replyB.strip():
        idsB = tokenizer.encode(replyB, return_tensors="pt")
        if idsB.shape[-1] > 0:
            lossA = agentB(idsB, labels=idsB).loss
            rewardA = shaped_reward(replyB, lossA)

    return replyA, replyB, lossA, lossB, rewardA, rewardB, history

# --- Training Loop ---
history = "Hello" + tokenizer.eos_token
baselineA, baselineB = 0.0, 0.0

for step in range(20):
    replyA, replyB, lossA, lossB, rewardA, rewardB, history = dialogue_step(agentA, agentB, history)

    if lossA is not None:
        baselineA = 0.9 * baselineA + 0.1 * rewardA
        optA.zero_grad()
        (lossA * (rewardA - baselineA)).backward()
        torch.nn.utils.clip_grad_norm_(agentA.parameters(), 1.0)
        optA.step()

    if lossB is not None:
        baselineB = 0.9 * baselineB + 0.1 * rewardB
        optB.zero_grad()
        (lossB * (rewardB - baselineB)).backward()
        torch.nn.utils.clip_grad_norm_(agentB.parameters(), 1.0)
        optB.step()

    print(f"Step {step:02d}")
    print(f" A: {replyA}")
    print(f" B: {replyB}")
    print(f" RewardA={rewardA:.4f} | RewardB={rewardB:.4f}\n")

    # Trim history to avoid GPU blow-up
    if len(history) > 500:
        history = history[1:-1]
```
